<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="1.Transformer的组成 transformer由Encoder和Decoder组成 Encoder和Decoder都由六个block组成  2.Transform结构第一步： 获取输入句子的每一个单词的表示向量 X X的组成 单词的 Embedding  单词位置的 Embedding  相加     第二步： 将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 x) 传入">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/2023/08/23/Transformer%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1.Transformer的组成 transformer由Encoder和Decoder组成 Encoder和Decoder都由六个block组成  2.Transform结构第一步： 获取输入句子的每一个单词的表示向量 X X的组成 单词的 Embedding  单词位置的 Embedding  相加     第二步： 将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 x) 传入">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687586822899-ea63a5d0-ed24-44e9-bfea-65731c0b3095.png#averageHue=%23f5f4f0&clientId=uc10193ce-95f3-4&from=paste&height=331&id=u0e704e07&originHeight=414&originWidth=894&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=165560&status=done&style=none&taskId=u8dac93ed-9571-47a5-9550-fd78457e8b8&title=&width=715.2">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687586949149-2085a3fd-dc9a-4b30-b92f-ffedcb140986.png#averageHue=%23f0f0ec&clientId=uc10193ce-95f3-4&from=paste&height=486&id=u33b219f4&originHeight=608&originWidth=417&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=118839&status=done&style=none&taskId=u9050e70a-ae7c-490b-8b3d-b89f74f8eb1&title=&width=333.6">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687587236393-5f2e81ca-2a16-4bbd-8d78-24001618effb.png#averageHue=%23dbd4cb&clientId=uc10193ce-95f3-4&from=paste&height=378&id=u305ca37a&originHeight=473&originWidth=754&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=199809&status=done&style=none&taskId=u4ae600df-2e3c-419c-918d-8ed4e337797&title=&width=603.2">
<meta property="og:image" content="https://cdn.nlark.com/yuque/__latex/39555d1ef64a1f0de94951c1e638a3a5.svg#card=math&code=PE%28pos%2C2i%29%3Dsin%28%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd%7D%7D%7D%29%0A%5Cnewline%20--------------%0A%5Cnewline%0APE%28pos%2C2i%2B1%29%3Dcos%28%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd%7D%7D%7D%29%0A&id=jAoEQ">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687593801627-8013215d-3c81-4b11-8b94-e79c9c5af236.png#averageHue=%23ede7e4&clientId=u7bf22268-aa07-4&from=paste&height=437&id=uc71cd0cd&originHeight=770&originWidth=521&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=223029&status=done&style=none&taskId=u0eab48f4-f5e3-4d67-aa8a-fca00b0fbcc&title=&width=295.8000183105469">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687588810618-d665ef9b-282f-4ad1-9e9a-8b52f21e4935.png#averageHue=%23f5f5f4&clientId=uc10193ce-95f3-4&from=paste&height=307&id=u7310a88b&originHeight=513&originWidth=372&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=85572&status=done&style=none&taskId=uc2e319a5-1219-421d-85d8-b7ebd7217bb&title=&width=222.60000610351562">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589064296-4652bbbf-9c11-4745-b6ce-23a9baad2771.png#averageHue=%23e6e2ba&clientId=uc10193ce-95f3-4&from=paste&height=473&id=u4e9e8abc&originHeight=591&originWidth=417&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=96552&status=done&style=none&taskId=u5435c680-1d6e-4d62-b1f1-e5bb5282e8e&title=&width=333.6">
<meta property="og:image" content="https://cdn.nlark.com/yuque/__latex/5c132deec7a0aea90dddfe7d6d4adee9.svg#card=math&code=Attention%28Q%2CK%2CV%29%3Dsoftmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V%20%20%5C%5C%0A-%20%5C%5C%0Ad_k%E6%98%AFQ%EF%BC%8CK%E7%9F%A9%E9%98%B5%E7%9A%84%E5%88%97%E6%95%B0%EF%BC%8C%E5%8D%B3%E5%90%91%E9%87%8F%E7%BB%B4%E5%BA%A6&id=udSFn">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589560745-2f75acaf-5b53-40d1-bc0b-15f1bd770a4c.png#averageHue=%23f3e8dd&clientId=uc10193ce-95f3-4&from=paste&height=214&id=uddea5954&originHeight=267&originWidth=679&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=31637&status=done&style=none&taskId=ua36beaa9-e91c-481b-8020-1b217e88c47&title=&width=543.2">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589696138-b6106e59-9c7c-426b-94ef-24934f3bef49.png#averageHue=%23cdbcba&clientId=uc10193ce-95f3-4&from=paste&height=164&id=u3bbee219&originHeight=205&originWidth=650&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=22494&status=done&style=none&taskId=ud5053404-02de-4470-81e1-b2f46e4a148&title=&width=520">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589736601-1c1426d9-f3d1-4394-8281-c04c541a6b0d.png#averageHue=%23eddbd8&clientId=uc10193ce-95f3-4&from=paste&height=194&id=u4763b7fa&originHeight=272&originWidth=654&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=50756&status=done&style=none&taskId=u5ed08f8b-a1f2-4cde-91e1-61b12a18de5&title=&width=467.20001220703125">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589928524-9968140a-3ab0-4a1a-bda5-c2274dbd4c3c.png#averageHue=%23c8c5ad&clientId=uc10193ce-95f3-4&from=paste&height=466&id=u1508b6c2&originHeight=582&originWidth=425&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=110973&status=done&style=none&taskId=ufe01ef08-854d-4182-ab5e-445ad1a6f7d&title=&width=340">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687619200761-0475a0c9-e971-4e3c-8551-7cc34e369f3a.png#averageHue=%23fcfbfb&clientId=udcdd0a7a-3a37-4&from=paste&height=209&id=u2ec32694&originHeight=343&originWidth=731&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=40975&status=done&style=none&taskId=ua173a1e3-f561-453d-b2bb-53908636f8b&title=&width=446">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687619219133-7da2812b-ade0-4647-88c3-89cd0cda584f.png#averageHue=%23fefefd&clientId=udcdd0a7a-3a37-4&from=paste&id=u0492260b&originHeight=605&originWidth=1080&originalType=url&ratio=1.25&rotation=0&showTitle=false&size=155745&status=done&style=none&taskId=uba86ec1d-eb89-4cd6-a438-c3a317d3dbc&title=">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589983334-615fc7ca-534c-433d-87c4-ce03d590c7e1.png#averageHue=%23f9f4f4&clientId=uc10193ce-95f3-4&from=paste&height=443&id=u6cd7828f&originHeight=554&originWidth=442&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=90559&status=done&style=none&taskId=u7a3ab1eb-3cde-4cfc-af78-136c70331ba&title=&width=353.6">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687590044099-54bea6ac-5628-40b2-afb2-4bce396ce511.png#averageHue=%23f9f2f2&clientId=uc10193ce-95f3-4&from=paste&height=263&id=u5317452e&originHeight=449&originWidth=677&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=88082&status=done&style=none&taskId=u224b45c1-83df-4d80-b12a-6871c22752a&title=&width=396.6000061035156">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687593698224-72b814ed-56fe-4180-b957-1406c605696f.png#averageHue=%23ebe7e4&clientId=u7bf22268-aa07-4&from=paste&height=459&id=u174efd34&originHeight=580&originWidth=381&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=140250&status=done&style=none&taskId=uf7050c8f-41e3-4088-9f13-7235a794aec&title=&width=301.8000183105469">
<meta property="og:image" content="https://cdn.nlark.com/yuque/__latex/81a28dfe1aef958a7056cfc9ae7a4d6f.svg#card=math&code=LayerNorm%28X%2BMultiHeadAttention%28X%29%29%20%5C%5C%0A-%0A%5C%5C%0ALayerNorm%28X%2BFeedForward%28X%29%29&id=yeHbM">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687594063741-d0385cb8-1b07-4162-99f5-e4cf39f1779c.png#averageHue=%23f4f4f4&clientId=u7bf22268-aa07-4&from=paste&height=99&id=u38483391&originHeight=164&originWidth=631&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=27295&status=done&style=none&taskId=u360d0476-c195-43d7-9791-0e3613b162e&title=&width=379.8000183105469">
<meta property="og:image" content="https://cdn.nlark.com/yuque/__latex/26e36df05d241eb9b17d93805ae11e86.svg#card=math&code=max%280%2CXW_1%2Bb_1%29W2%2Bb_2&id=T56Dz">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687594701210-a5c0773d-f20e-4e60-8cfd-6cf8dbb4cac5.png#averageHue=%23ecece9&clientId=u7bf22268-aa07-4&from=paste&height=476&id=ub96579f0&originHeight=799&originWidth=540&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=184135&status=done&style=none&taskId=ufb46f65c-d781-4e74-855c-693223688d8&title=&width=322">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687594858645-8a0a24e3-d19e-40f5-a47b-56b8b87f7fb6.png#averageHue=%23ebe7e4&clientId=u7bf22268-aa07-4&from=paste&height=515&id=u4b5948e4&originHeight=774&originWidth=505&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=222598&status=done&style=none&taskId=u27c04c7d-7ae0-49b2-94bc-4ebce436ff0&title=&width=336">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595160962-bdaf71a5-b099-4dac-8ac2-4db6885f11b2.png#averageHue=%23f8f6f5&clientId=u7bf22268-aa07-4&from=paste&height=133&id=u26a6fdda&originHeight=199&originWidth=752&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=73395&status=done&style=none&taskId=u13df1f15-c82f-47d4-8d37-32e14b4f839&title=&width=501.60003662109375">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595301793-c8d3149e-8345-4114-9fcd-a16cba2095f7.png#averageHue=%23f1f219&clientId=u7bf22268-aa07-4&from=paste&height=256&id=u9dc1ecea&originHeight=367&originWidth=765&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=56023&status=done&style=none&taskId=udd5920be-697f-4bb5-9691-6b8ec0645b8&title=&width=534">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595354551-53bab1be-fab0-4014-ac0c-234a878ed8f4.png#averageHue=%23f2e6db&clientId=u7bf22268-aa07-4&from=paste&height=230&id=ue3371830&originHeight=335&originWidth=744&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=89480&status=done&style=none&taskId=u997c86d3-dac9-4645-8e11-37f3bfb1ce0&title=&width=511.20001220703125">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595388970-4deccea9-f01f-416f-ae1f-2bceb13f69c3.png#averageHue=%23e8ded3&clientId=u7bf22268-aa07-4&from=paste&height=194&id=udd33df99&originHeight=292&originWidth=759&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=98563&status=done&style=none&taskId=u0871b548-9bf9-4ca3-9fef-33d03fa4ad5&title=&width=504.20001220703125">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595459172-d0182ae5-bd47-4143-b6d5-287f2d72dfe5.png#averageHue=%23e5d6d5&clientId=u7bf22268-aa07-4&from=paste&height=211&id=uc823af2d&originHeight=370&originWidth=762&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=35381&status=done&style=none&taskId=uc5eaa350-be7c-4329-aba0-ad8b464447a&title=&width=434.6000061035156">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595699637-890470dd-7810-4665-91c9-fc0b882cf216.png#averageHue=%23fbfaf4&clientId=u7bf22268-aa07-4&from=paste&height=180&id=ua943095c&originHeight=316&originWidth=731&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=99094&status=done&style=none&taskId=ueb2f23fa-673e-421e-b6f0-f5677cc6005&title=&width=416.8000183105469">
<meta property="og:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595766629-c1696b5e-2d4e-4400-95f4-c808abc4abf1.png#averageHue=%23fdf9f2&clientId=u7bf22268-aa07-4&from=paste&height=310&id=uac7a967b&originHeight=453&originWidth=736&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=89797&status=done&style=none&taskId=ua61340da-45e2-44af-b31c-91c2515cf55&title=&width=503.79998779296875">
<meta property="article:published_time" content="2023-08-23T06:42:45.482Z">
<meta property="article:modified_time" content="2023-08-23T06:41:53.194Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687586822899-ea63a5d0-ed24-44e9-bfea-65731c0b3095.png#averageHue=%23f5f4f0&clientId=uc10193ce-95f3-4&from=paste&height=331&id=u0e704e07&originHeight=414&originWidth=894&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=165560&status=done&style=none&taskId=u8dac93ed-9571-47a5-9550-fd78457e8b8&title=&width=715.2">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/2023/08/23/Transformer模型/"/>





  <title> | Hexo</title>
  








<meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/08/23/Transformer%E6%A8%A1%E5%9E%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline"></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2023-08-23T14:42:45+08:00">
                2023-08-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="1-Transformer的组成"><a href="#1-Transformer的组成" class="headerlink" title="1.Transformer的组成"></a>1.Transformer的组成</h2><ul>
<li>transformer由Encoder和Decoder组成</li>
<li>Encoder和Decoder都由六个block组成</li>
</ul>
<h2 id="2-Transform结构"><a href="#2-Transform结构" class="headerlink" title="2.Transform结构"></a>2.Transform结构</h2><h3 id="第一步："><a href="#第一步：" class="headerlink" title="第一步："></a>第一步：</h3><ul>
<li>获取输入句子的每一个单词的表示向量 <strong>X</strong></li>
<li><strong>X</strong>的组成<ul>
<li>单词的 Embedding </li>
<li>单词位置的 Embedding </li>
<li>相加</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687586822899-ea63a5d0-ed24-44e9-bfea-65731c0b3095.png#averageHue=%23f5f4f0&clientId=uc10193ce-95f3-4&from=paste&height=331&id=u0e704e07&originHeight=414&originWidth=894&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=165560&status=done&style=none&taskId=u8dac93ed-9571-47a5-9550-fd78457e8b8&title=&width=715.2" alt="image.png"></p>
<h3 id="第二步："><a href="#第二步：" class="headerlink" title="第二步："></a>第二步：</h3><ul>
<li>将得到的单词表示向量矩阵 (如上图所示，每一行是一个单词的表示 <strong>x</strong>) 传入 Encoder 中</li>
<li>经过 6 个 Encoder block 后可以得到句子所有单词的编码信息矩阵 <strong>C</strong>，如下图。</li>
<li>单词向量矩阵用 **X(n×d)**表示<ul>
<li>n 是句子中单词个数</li>
<li>d 是表示向量的维度 (论文中 d&#x3D;512)。</li>
</ul>
</li>
<li>每一个 Encoder block 输出的矩阵维度与输入完全一致</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687586949149-2085a3fd-dc9a-4b30-b92f-ffedcb140986.png#averageHue=%23f0f0ec&clientId=uc10193ce-95f3-4&from=paste&height=486&id=u33b219f4&originHeight=608&originWidth=417&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=118839&status=done&style=none&taskId=u9050e70a-ae7c-490b-8b3d-b89f74f8eb1&title=&width=333.6" alt="image.png"></p>
<h3 id="第三步"><a href="#第三步" class="headerlink" title="第三步"></a>第三步</h3><ul>
<li>将 Encoder 输出的编码信息矩阵 <strong>C</strong>传递到 Decoder </li>
<li>Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。</li>
<li>在使用的过程中，翻译到单词 i+1 的时候需要通过 <strong>Mask (掩盖)</strong> 操作遮盖住 i+1 之后的单词。</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687587236393-5f2e81ca-2a16-4bbd-8d78-24001618effb.png#averageHue=%23dbd4cb&clientId=uc10193ce-95f3-4&from=paste&height=378&id=u305ca37a&originHeight=473&originWidth=754&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=199809&status=done&style=none&taskId=u4ae600df-2e3c-419c-918d-8ed4e337797&title=&width=603.2" alt="image.png"></p>
<ul>
<li>上图 Decoder 接收了 Encoder 的编码矩阵 C</li>
<li>然后首先输入一个翻译开始符 “<Begin>“预测第一个单词 “I”；</li>
<li>然后输入翻译开始符 “<Begin>“ 和单词 “I”，预测单词 “have”，以此类推</li>
<li>这是 Transformer 使用时候的大致流程，接下来是里面各个部分的细节</li>
</ul>
<h2 id="3-Transform的输入"><a href="#3-Transform的输入" class="headerlink" title="3.Transform的输入"></a>3.Transform的输入</h2><ul>
<li>Transformer 中单词的输入表示 x由单词 Embedding 和位置 Embedding 相加得到。</li>
</ul>
<h3 id="3-1单词Embedding"><a href="#3-1单词Embedding" class="headerlink" title="3.1单词Embedding"></a>3.1单词Embedding</h3><ul>
<li>单词的 Embedding 有很多种方式可以获取</li>
<li>例如可以采用 Word2Vec、Glove 等算法预训练得到</li>
<li>也可以在 Transformer 中训练得到</li>
</ul>
<h3 id="3-2位置的Embedding"><a href="#3-2位置的Embedding" class="headerlink" title="3.2位置的Embedding"></a>3.2位置的Embedding</h3><ul>
<li><strong>因为 Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息，而这部分信息对于 NLP 来说非常重要。</strong></li>
<li>所以 Transformer 中使用位置 Embedding 保存单词在序列中的<strong>相对或绝对位置。</strong></li>
<li>位置 Embedding 用 <strong>PE</strong>表示，**PE **的维度与单词 Embedding 是一样的。</li>
<li>**PE **可以通过训练得到，也可以使用某种公式计算得到。在 Transformer 中采用了后者，计算公式如下：</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/__latex/39555d1ef64a1f0de94951c1e638a3a5.svg#card=math&code=PE%28pos%2C2i%29%3Dsin%28%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd%7D%7D%7D%29%0A%5Cnewline%20--------------%0A%5Cnewline%0APE%28pos%2C2i%2B1%29%3Dcos%28%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd%7D%7D%7D%29%0A&id=jAoEQ"></p>
<blockquote>
<p>其中，pos 表示单词在句子中的位置<br>d 表示 PE的维度 (与词 Embedding 一样)<br>2i 表示偶数的维度，2i+1 表示奇数维度 (即 2i≤d, 2i+1≤d)</p>
</blockquote>
<ul>
<li>优点<ul>
<li>使 **PE **能够适应比训练集里面所有句子更长的句子，假设训练集里面最长的句子是有 20 个单词，突然来了一个长度为 21 的句子，则使用公式计算的方法可以计算出第 21 位的 Embedding</li>
<li>可以让模型容易地计算出相对位置，对于固定长度的间距 k，PE(pos+k) 可以用 PE(pos) 计算得到。因为 Sin(A+B) &#x3D; Sin(A)Cos(B) + Cos(A)Sin(B), Cos(A+B) &#x3D; Cos(A)Cos(B) - Sin(A)Sin(B)</li>
</ul>
</li>
<li>将单词的词 Embedding 和位置 Embedding 相加，就可以得到单词的表示向量 x，x 就是 Transformer 的输入</li>
</ul>
<h2 id="4-Self-Attention"><a href="#4-Self-Attention" class="headerlink" title="4.Self-Attention"></a>4.Self-Attention</h2><h3 id="4-1整体细化结构图"><a href="#4-1整体细化结构图" class="headerlink" title="4.1整体细化结构图"></a>4.1整体细化结构图</h3><p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687593801627-8013215d-3c81-4b11-8b94-e79c9c5af236.png#averageHue=%23ede7e4&clientId=u7bf22268-aa07-4&from=paste&height=437&id=uc71cd0cd&originHeight=770&originWidth=521&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=223029&status=done&style=none&taskId=u0eab48f4-f5e3-4d67-aa8a-fca00b0fbcc&title=&width=295.8000183105469" alt="image.png"></p>
<h3 id="4-2Transform细化结构说明"><a href="#4-2Transform细化结构说明" class="headerlink" title="4.2Transform细化结构说明"></a>4.2Transform细化结构说明</h3><ul>
<li>上图是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block。</li>
<li>红色圈中的部分为 <strong>Multi-Head Attention</strong>，是由多个 <strong>Self-Attention</strong>组成的，可以看到 Encoder block 包含一个 Multi-Head Attention</li>
<li>而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)</li>
<li>Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，<strong>Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。</strong></li>
</ul>
<h3 id="4-3-Self-Attention-结构"><a href="#4-3-Self-Attention-结构" class="headerlink" title="4.3  Self-Attention 结构"></a>4.3  Self-Attention 结构</h3><p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687588810618-d665ef9b-282f-4ad1-9e9a-8b52f21e4935.png#averageHue=%23f5f5f4&clientId=uc10193ce-95f3-4&from=paste&height=307&id=u7310a88b&originHeight=513&originWidth=372&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=85572&status=done&style=none&taskId=uc2e319a5-1219-421d-85d8-b7ebd7217bb&title=&width=222.60000610351562" alt="image.png"></p>
<ul>
<li>上图是 Self-Attention 的结构，在计算的时候需要用到矩阵<ul>
<li><strong>Q</strong>(查询)</li>
<li><strong>K</strong>(键值)</li>
<li><strong>V</strong>(值)</li>
</ul>
</li>
<li>在实际中，Self-Attention 接收的是输入(单词的表示向量 <strong>x</strong>组成的矩阵 <strong>X</strong>) 或者上一个 Encoder block 的输出。</li>
<li>而 <strong>Q</strong>, <strong>K</strong>, <strong>V</strong> 正是通过 Self-Attention 的输入进行线性变换得到的。</li>
</ul>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>4.4Q，K，V的计算</p>
<ul>
<li>Self-Attention 的输入用矩阵 <strong>X</strong>进行表示</li>
<li>则可以使用<strong>线性变阵矩阵</strong> <strong>WQ</strong>, <strong>WK</strong>, <strong>WV</strong> 计算得到 <strong>Q</strong>, <strong>K</strong>, <strong>V</strong></li>
<li>计算如下图所示，<strong>注意 X, Q, K, V 的每一行都表示一个单词</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589064296-4652bbbf-9c11-4745-b6ce-23a9baad2771.png#averageHue=%23e6e2ba&clientId=uc10193ce-95f3-4&from=paste&height=473&id=u4e9e8abc&originHeight=591&originWidth=417&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=96552&status=done&style=none&taskId=u5435c680-1d6e-4d62-b1f1-e5bb5282e8e&title=&width=333.6" alt="image.png"></p>
<h3 id="4-5Self-Attention的输出"><a href="#4-5Self-Attention的输出" class="headerlink" title="4.5Self-Attention的输出"></a>4.5Self-Attention的输出</h3><ul>
<li>得到Q，K，V之后就可以计算Self-Attention的输出了，计算公式如下</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/__latex/5c132deec7a0aea90dddfe7d6d4adee9.svg#card=math&code=Attention%28Q%2CK%2CV%29%3Dsoftmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V%20%20%5C%5C%0A-%20%5C%5C%0Ad_k%E6%98%AFQ%EF%BC%8CK%E7%9F%A9%E9%98%B5%E7%9A%84%E5%88%97%E6%95%B0%EF%BC%8C%E5%8D%B3%E5%90%91%E9%87%8F%E7%BB%B4%E5%BA%A6&id=udSFn"></p>
<ul>
<li>将query和key之间两两计算相似度，依据相似度对各个value进行加权</li>
<li>公式中计算矩阵 <strong>Q</strong>和 <strong>K</strong> 每一行向量的<strong>内积</strong>，向量维度比较长，则计算的值的绝对大小差距会很大，Softmax之后可能会出现有的值极度趋近1，其余值极度趋近0的情况。这会导致梯度比较小，不利于网络的训练</li>
<li>要求<strong>Q</strong>与<strong>K</strong>的单个向量是等长的，对这两个向量计算内积，也就是余弦相似度，如果两个向量正交，则内积为0，也就是不相关；反之，如果相关，则二者内积就会很大</li>
<li><strong>Q</strong> 乘以 <strong>K</strong> 的转置后，得到的矩阵行列数都为 n</li>
</ul>
<p><strong>对于下面的例子而言：</strong></p>
<ul>
<li>n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度</li>
<li>下图为 <strong>Q</strong> 乘以 <strong>K</strong> 的转置，1234 表示的是句子中的单词</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589560745-2f75acaf-5b53-40d1-bc0b-15f1bd770a4c.png#averageHue=%23f3e8dd&clientId=uc10193ce-95f3-4&from=paste&height=214&id=uddea5954&originHeight=267&originWidth=679&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=31637&status=done&style=none&taskId=ua36beaa9-e91c-481b-8020-1b217e88c47&title=&width=543.2" alt="image.png"></p>
<ul>
<li>得到 <strong>QK</strong>T（内积） 之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数</li>
<li>公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1</li>
<li>得到 Softmax 矩阵之后可以和 <strong>V</strong>相乘，得到最终的输出 <strong>Z</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589696138-b6106e59-9c7c-426b-94ef-24934f3bef49.png#averageHue=%23cdbcba&clientId=uc10193ce-95f3-4&from=paste&height=164&id=u3bbee219&originHeight=205&originWidth=650&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=22494&status=done&style=none&taskId=ud5053404-02de-4470-81e1-b2f46e4a148&title=&width=520" alt="image.png"></p>
<ul>
<li>上图中 Softmax 矩阵的第 1 行表示单词 1 与其他所有单词的 attention 系数</li>
<li>最终单词 1 的输出 <strong>Z</strong>1 等于所有单词 i 的值 <strong>V</strong>i 根据 attention 系数的比例加在一起得到，如下图所示：</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589736601-1c1426d9-f3d1-4394-8281-c04c541a6b0d.png#averageHue=%23eddbd8&clientId=uc10193ce-95f3-4&from=paste&height=194&id=u4763b7fa&originHeight=272&originWidth=654&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=50756&status=done&style=none&taskId=u5ed08f8b-a1f2-4cde-91e1-61b12a18de5&title=&width=467.20001220703125" alt="image.png"></p>
<h3 id="4-6Multi-Head-Attention"><a href="#4-6Multi-Head-Attention" class="headerlink" title="4.6Multi-Head Attention"></a>4.6Multi-Head Attention</h3><ul>
<li>在上一步，我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 <strong>Z</strong>，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589928524-9968140a-3ab0-4a1a-bda5-c2274dbd4c3c.png#averageHue=%23c8c5ad&clientId=uc10193ce-95f3-4&from=paste&height=466&id=u1508b6c2&originHeight=582&originWidth=425&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=110973&status=done&style=none&taskId=ufe01ef08-854d-4182-ab5e-445ad1a6f7d&title=&width=340" alt="image.png"></p>
<ul>
<li>从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层</li>
<li>首先将输入 <strong>X</strong>分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵 <strong>Z</strong></li>
<li>下图是 h&#x3D;8 时候的情况，此时会得到 8 个输出矩阵 <strong>Z</strong></li>
<li>公式如下</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687619200761-0475a0c9-e971-4e3c-8551-7cc34e369f3a.png#averageHue=%23fcfbfb&clientId=udcdd0a7a-3a37-4&from=paste&height=209&id=u2ec32694&originHeight=343&originWidth=731&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=40975&status=done&style=none&taskId=ua173a1e3-f561-453d-b2bb-53908636f8b&title=&width=446" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687619219133-7da2812b-ade0-4647-88c3-89cd0cda584f.png#averageHue=%23fefefd&clientId=udcdd0a7a-3a37-4&from=paste&id=u0492260b&originHeight=605&originWidth=1080&originalType=url&ratio=1.25&rotation=0&showTitle=false&size=155745&status=done&style=none&taskId=uba86ec1d-eb89-4cd6-a438-c3a317d3dbc&title=" alt="image.png"><br><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687589983334-615fc7ca-534c-433d-87c4-ce03d590c7e1.png#averageHue=%23f9f4f4&clientId=uc10193ce-95f3-4&from=paste&height=443&id=u6cd7828f&originHeight=554&originWidth=442&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=90559&status=done&style=none&taskId=u7a3ab1eb-3cde-4cfc-af78-136c70331ba&title=&width=353.6" alt="image.png"></p>
<ul>
<li>得到 8 个输出矩阵 <strong>Z</strong>1 到 <strong>Z</strong>8 之后，Multi-Head Attention 将它们拼接在一起 (<strong>Concat</strong>)</li>
<li>然后传入一个 <strong>Linear</strong>层，得到 Multi-Head Attention 最终的输出 <strong>Z</strong></li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687590044099-54bea6ac-5628-40b2-afb2-4bce396ce511.png#averageHue=%23f9f2f2&clientId=uc10193ce-95f3-4&from=paste&height=263&id=u5317452e&originHeight=449&originWidth=677&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=88082&status=done&style=none&taskId=u224b45c1-83df-4d80-b12a-6871c22752a&title=&width=396.6000061035156" alt="image.png"></p>
<ul>
<li>可以看到Multi-Head Attention输出的矩阵<strong>Z</strong>与其输入的矩阵<strong>X</strong>的维度是一样的</li>
</ul>
<h2 id="5-Encoder结构"><a href="#5-Encoder结构" class="headerlink" title="5.Encoder结构"></a>5.Encoder结构</h2><h3 id="5-1结构图"><a href="#5-1结构图" class="headerlink" title="5.1结构图"></a>5.1结构图</h3><p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687593698224-72b814ed-56fe-4180-b957-1406c605696f.png#averageHue=%23ebe7e4&clientId=u7bf22268-aa07-4&from=paste&height=459&id=u174efd34&originHeight=580&originWidth=381&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=140250&status=done&style=none&taskId=uf7050c8f-41e3-4088-9f13-7235a794aec&title=&width=301.8000183105469" alt="image.png"></p>
<h3 id="5-2-Add-Norm结构解释"><a href="#5-2-Add-Norm结构解释" class="headerlink" title="5.2 Add&amp;Norm结构解释"></a>5.2 Add&amp;Norm结构解释</h3><ul>
<li>上图红色部分是 Transformer 的 Encoder block 结构，可以看到是由 Multi-Head Attention, Add &amp; Norm, Feed Forward, Add &amp; Norm 组成的。</li>
<li>Add &amp; Norm层由Add和Norm两部分组成，计算公式如下：</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/__latex/81a28dfe1aef958a7056cfc9ae7a4d6f.svg#card=math&code=LayerNorm%28X%2BMultiHeadAttention%28X%29%29%20%5C%5C%0A-%0A%5C%5C%0ALayerNorm%28X%2BFeedForward%28X%29%29&id=yeHbM"></p>
<ul>
<li>其中 <strong>X</strong>表示 Multi-Head Attention 或者 Feed Forward 的输入</li>
<li>MultiHeadAttention(<strong>X</strong>) 和 FeedForward(<strong>X</strong>) 表示输出 (输出与输入 <strong>X</strong> 维度是一样的，所以可以相加)。</li>
<li><strong>Add</strong>指 <strong>X</strong>+MultiHeadAttention(<strong>X</strong>)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分，在 ResNet 中经常用到：</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687594063741-d0385cb8-1b07-4162-99f5-e4cf39f1779c.png#averageHue=%23f4f4f4&clientId=u7bf22268-aa07-4&from=paste&height=99&id=u38483391&originHeight=164&originWidth=631&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=27295&status=done&style=none&taskId=u360d0476-c195-43d7-9791-0e3613b162e&title=&width=379.8000183105469" alt="image.png"></p>
<ul>
<li><strong>Norm</strong>指 Layer Normalization，通常用于 RNN 结构</li>
<li>Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛</li>
</ul>
<h3 id="5-3-Feed-Forward"><a href="#5-3-Feed-Forward" class="headerlink" title="5.3 Feed Forward"></a>5.3 Feed Forward</h3><ul>
<li>Feed Forward 层比较简单，是一个两层的全连接层<ul>
<li>第一层的激活函数为 Relu</li>
<li>第二层不使用激活函数，对应的公式如下：</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/__latex/26e36df05d241eb9b17d93805ae11e86.svg#card=math&code=max%280%2CXW_1%2Bb_1%29W2%2Bb_2&id=T56Dz"></p>
<ul>
<li><strong>X</strong>是输入，Feed Forward 最终得到的输出矩阵的维度与 <strong>X</strong> 一致。</li>
</ul>
<h3 id="5-4-组成Encoder"><a href="#5-4-组成Encoder" class="headerlink" title="5.4 组成Encoder"></a>5.4 组成Encoder</h3><ul>
<li>通过上面描述的 <ul>
<li>Multi-Head Attention</li>
<li>Feed Forward</li>
<li>Add &amp; Norm</li>
</ul>
</li>
<li>就可以构造出一个 Encoder block，Encoder block 接收输入矩阵 <strong>X</strong>(n×d)，并输出一个矩阵 <strong>O</strong>(n×d)。</li>
<li>通过多个 Encoder block 叠加就可以组成 Encoder<ul>
<li>第一个 Encoder block 的输入为句子单词的表示向量矩阵</li>
<li>后续 Encoder block 的输入是前一个 Encoder block 的输出</li>
<li>最后一个 Encoder block 输出的矩阵就是 <strong>编码信息矩阵 C</strong></li>
</ul>
</li>
<li>这一矩阵后续会用到 Decoder 中</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687594701210-a5c0773d-f20e-4e60-8cfd-6cf8dbb4cac5.png#averageHue=%23ecece9&clientId=u7bf22268-aa07-4&from=paste&height=476&id=ub96579f0&originHeight=799&originWidth=540&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=184135&status=done&style=none&taskId=ufb46f65c-d781-4e74-855c-693223688d8&title=&width=322" alt="image.png"></p>
<h2 id="6-Decoder结构"><a href="#6-Decoder结构" class="headerlink" title="6.Decoder结构"></a>6.Decoder结构</h2><h3 id="6-1结构图"><a href="#6-1结构图" class="headerlink" title="6.1结构图"></a>6.1结构图</h3><p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687594858645-8a0a24e3-d19e-40f5-a47b-56b8b87f7fb6.png#averageHue=%23ebe7e4&clientId=u7bf22268-aa07-4&from=paste&height=515&id=u4b5948e4&originHeight=774&originWidth=505&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=222598&status=done&style=none&taskId=u27c04c7d-7ae0-49b2-94bc-4ebce436ff0&title=&width=336" alt="image.png"></p>
<ul>
<li>上图红色部分为 Transformer 的 Decoder block 结构，与 Encoder block 相似，但是存在一些区别：<ul>
<li>包含两个 Multi-Head Attention 层。</li>
<li>第一个 Multi-Head Attention 层采用了 Masked 操作。</li>
<li>第二个 Multi-Head Attention 层的 <strong>K</strong>, <strong>V</strong> 矩阵使用 Encoder 的编码信息矩阵 <strong>C</strong> 进行计算，而 <strong>Q</strong> 使用上一个 Decoder block 的输出计算。</li>
<li>最后有一个 Softmax 层计算下一个翻译单词的概率。</li>
</ul>
</li>
</ul>
<h3 id="6-2第一个Multi-Head-Attention"><a href="#6-2第一个Multi-Head-Attention" class="headerlink" title="6.2第一个Multi-Head Attention"></a>6.2第一个Multi-Head Attention</h3><ul>
<li>Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作<ul>
<li>因为在翻译的过程中是顺序翻译的，即翻译完第 i 个单词，才可以翻译第 i+1 个单词。</li>
</ul>
</li>
<li>通过 Masked 操作可以防止第 i 个单词知道 i+1 个单词之后的信息。</li>
<li>下面以 “我有一只猫” 翻译成 “I have a cat” 为例，了解一下 Masked 操作。</li>
<li>下面的描述中使用了类似 Teacher Forcing 的概念。</li>
<li>在 Decoder 的时候，是需要根据之前的翻译，求解当前最有可能的翻译，如下图所示。首先根据输入 “<Begin>“ 预测出第一个单词为 “I”，然后根据输入 “<Begin> I” 预测下一个单词 “have”</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595160962-bdaf71a5-b099-4dac-8ac2-4db6885f11b2.png#averageHue=%23f8f6f5&clientId=u7bf22268-aa07-4&from=paste&height=133&id=u26a6fdda&originHeight=199&originWidth=752&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=73395&status=done&style=none&taskId=u13df1f15-c82f-47d4-8d37-32e14b4f839&title=&width=501.60003662109375" alt="image.png"></p>
<ul>
<li>Decoder 可以在训练的过程中使用 Teacher Forcing 并且并行化训练，即将正确的单词序列 (<Begin> I have a cat) 和对应输出 (I have a cat <end>) 传递到 Decoder。</li>
<li>那么在预测第 i 个输出时，就要将第 i+1 之后的单词掩盖住，**注意 Mask 操作是在 Self-Attention 的 Softmax 之前使用的，下面用 0 1 2 3 4 5 分别表示 “<Begin> I have a cat <end>“**。</li>
</ul>
<h4 id="6-2-1实例预测过程"><a href="#6-2-1实例预测过程" class="headerlink" title="6.2.1实例预测过程"></a>6.2.1实例预测过程</h4><ul>
<li><strong>第一步：</strong>是 Decoder 的输入矩阵和 <strong>Mask</strong> 矩阵，输入矩阵包含 “<Begin> I have a cat” (0, 1, 2, 3, 4) 五个单词的表示向量，<strong>Mask</strong> 是一个 5×5 的矩阵。在 <strong>Mask</strong> 可以发现单词 0 只能使用单词 0 的信息，而单词 1 可以使用单词 0, 1 的信息，即只能使用之前的信息。</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595301793-c8d3149e-8345-4114-9fcd-a16cba2095f7.png#averageHue=%23f1f219&clientId=u7bf22268-aa07-4&from=paste&height=256&id=u9dc1ecea&originHeight=367&originWidth=765&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=56023&status=done&style=none&taskId=udd5920be-697f-4bb5-9691-6b8ec0645b8&title=&width=534" alt="image.png"></p>
<ul>
<li><strong>第二步：</strong>接下来的操作和之前的 Self-Attention 一样，通过输入矩阵 <strong>X</strong>计算得到 <strong>Q</strong>, <strong>K</strong>, <strong>V</strong> 矩阵。然后计算 <strong>Q</strong> 和 <strong>K</strong>T 的乘积 <strong>QK</strong>T。</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595354551-53bab1be-fab0-4014-ac0c-234a878ed8f4.png#averageHue=%23f2e6db&clientId=u7bf22268-aa07-4&from=paste&height=230&id=ue3371830&originHeight=335&originWidth=744&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=89480&status=done&style=none&taskId=u997c86d3-dac9-4645-8e11-37f3bfb1ce0&title=&width=511.20001220703125" alt="image.png"></p>
<ul>
<li><strong>第三步：</strong>在得到 <strong>QK</strong>T 之后需要进行 Softmax，计算 attention score，我们在 Softmax 之前需要使用 <strong>Mask</strong>矩阵遮挡住每一个单词之后的信息，遮挡操作如下:</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595388970-4deccea9-f01f-416f-ae1f-2bceb13f69c3.png#averageHue=%23e8ded3&clientId=u7bf22268-aa07-4&from=paste&height=194&id=udd33df99&originHeight=292&originWidth=759&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=98563&status=done&style=none&taskId=u0871b548-9bf9-4ca3-9fef-33d03fa4ad5&title=&width=504.20001220703125" alt="image.png"></p>
<ul>
<li>得到 <strong>Mask QK</strong>T 之后在 <strong>Mask QK</strong>T 上进行 Softmax，每一行的和都为 1。但是单词 0 在单词 1, 2, 3, 4 上的 attention score 都为 0。</li>
<li><strong>第四步：</strong>使用 <strong>Mask QK</strong>T 与矩阵 <strong>V</strong>相乘，得到输出 <strong>Z</strong>，则单词 1 的输出向量 <strong>Z</strong>1 是只包含单词 1 信息的。</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595459172-d0182ae5-bd47-4143-b6d5-287f2d72dfe5.png#averageHue=%23e5d6d5&clientId=u7bf22268-aa07-4&from=paste&height=211&id=uc823af2d&originHeight=370&originWidth=762&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=35381&status=done&style=none&taskId=uc5eaa350-be7c-4329-aba0-ad8b464447a&title=&width=434.6000061035156" alt="image.png"></p>
<ul>
<li><strong>第五步：</strong>通过上述步骤就可以得到一个 Mask Self-Attention 的输出矩阵 <strong>Z</strong>i，然后和 Encoder 类似，通过 Multi-Head Attention 拼接多个输出 <strong>Z</strong>i 然后计算得到第一个 Multi-Head Attention 的输出 <strong>Z</strong>，<strong>Z</strong>与输入 <strong>X</strong> 维度一样。</li>
</ul>
<h3 id="6-3第二个-Multi-Head-Attention"><a href="#6-3第二个-Multi-Head-Attention" class="headerlink" title="6.3第二个 Multi-Head Attention"></a>6.3第二个 Multi-Head Attention</h3><ul>
<li>Decoder block 第二个 Multi-Head Attention 变化不大，主要的区别在于其中 Self-Attention 的 <strong>K</strong>, <strong>V</strong>矩阵不是使用上一个 Decoder block 的输出计算的，而是使用 <strong>Encoder 的编码信息矩阵 C</strong> 计算的。</li>
<li>根据 Encoder 的输出 <strong>C</strong>计算得到 <strong>K</strong>, <strong>V</strong>，根据上一个 Multi-Head计算输出 <strong>Z</strong> 计算 <strong>Q</strong> (如果是第一个 Decoder block 则使用输入矩阵 <strong>X</strong> 进行计算)，后续的计算方法与之前描述的一致。</li>
<li>这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)</li>
</ul>
<h3 id="6-4-SoftMax预测输出单词"><a href="#6-4-SoftMax预测输出单词" class="headerlink" title="6.4 SoftMax预测输出单词"></a>6.4 SoftMax预测输出单词</h3><ul>
<li>Decoder block 最后的部分是利用 Softmax 预测下一个单词，在之前的网络层我们可以得到一个最终的输出 <strong>Z</strong>，因为 Mask 的存在，使得单词 0 的输出 <strong>Z</strong>0 只包含单词 0 的信息，如下</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595699637-890470dd-7810-4665-91c9-fc0b882cf216.png#averageHue=%23fbfaf4&clientId=u7bf22268-aa07-4&from=paste&height=180&id=ua943095c&originHeight=316&originWidth=731&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=99094&status=done&style=none&taskId=ueb2f23fa-673e-421e-b6f0-f5677cc6005&title=&width=416.8000183105469" alt="image.png"></p>
<ul>
<li>Softmax 根据输出矩阵的每一行预测下一个单词</li>
</ul>
<p><img src="https://cdn.nlark.com/yuque/0/2023/png/34805283/1687595766629-c1696b5e-2d4e-4400-95f4-c808abc4abf1.png#averageHue=%23fdf9f2&clientId=u7bf22268-aa07-4&from=paste&height=310&id=uac7a967b&originHeight=453&originWidth=736&originalType=binary&ratio=1.25&rotation=0&showTitle=false&size=89797&status=done&style=none&taskId=ua61340da-45e2-44af-b31c-91c2515cf55&title=&width=503.79998779296875" alt="image.png"></p>
<h2 id="7-Encoder和Decoder的区别"><a href="#7-Encoder和Decoder的区别" class="headerlink" title="7.Encoder和Decoder的区别"></a>7.Encoder和Decoder的区别</h2><ul>
<li>Encoder中只利用的自注意力和MLP，而Decoder还用到了互注意力模块；</li>
<li>Encoder的自注意力是不需要mask的，而Decoder的自注意力需要掩膜进行时序上的推理</li>
</ul>
<h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8.总结"></a>8.总结</h2><ul>
<li>Transformer 与 RNN 不同，可以比较好地并行训练。</li>
<li>Transformer 本身是不能利用单词的顺序信息的，因此需要在输入中添加位置 Embedding，否则 Transformer 就是一个词袋模型了。</li>
<li>Transformer 的重点是 Self-Attention 结构，其中用到的 <strong>Q</strong>, <strong>K</strong>, <strong>V</strong>矩阵通过输出进行线性变换得到。</li>
<li>Transformer 中 Multi-Head Attention 中有多个 Self-Attention，可以捕获单词之间多种维度上的相关系数 attention score</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2023/08/23/test-my-site/" rel="next" title="test_my_site">
                <i class="fa fa-chevron-left"></i> test_my_site
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Transformer%E7%9A%84%E7%BB%84%E6%88%90"><span class="nav-number">1.</span> <span class="nav-text">1.Transformer的组成</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Transform%E7%BB%93%E6%9E%84"><span class="nav-number">2.</span> <span class="nav-text">2.Transform结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A"><span class="nav-number">2.1.</span> <span class="nav-text">第一步：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A"><span class="nav-number">2.2.</span> <span class="nav-text">第二步：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5"><span class="nav-number">2.3.</span> <span class="nav-text">第三步</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Transform%E7%9A%84%E8%BE%93%E5%85%A5"><span class="nav-number">3.</span> <span class="nav-text">3.Transform的输入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1%E5%8D%95%E8%AF%8DEmbedding"><span class="nav-number">3.1.</span> <span class="nav-text">3.1单词Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2%E4%BD%8D%E7%BD%AE%E7%9A%84Embedding"><span class="nav-number">3.2.</span> <span class="nav-text">3.2位置的Embedding</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Self-Attention"><span class="nav-number">4.</span> <span class="nav-text">4.Self-Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1%E6%95%B4%E4%BD%93%E7%BB%86%E5%8C%96%E7%BB%93%E6%9E%84%E5%9B%BE"><span class="nav-number">4.1.</span> <span class="nav-text">4.1整体细化结构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2Transform%E7%BB%86%E5%8C%96%E7%BB%93%E6%9E%84%E8%AF%B4%E6%98%8E"><span class="nav-number">4.2.</span> <span class="nav-text">4.2Transform细化结构说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Self-Attention-%E7%BB%93%E6%9E%84"><span class="nav-number">4.3.</span> <span class="nav-text">4.3  Self-Attention 结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">4.4.</span> <span class="nav-text"></span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5Self-Attention%E7%9A%84%E8%BE%93%E5%87%BA"><span class="nav-number">4.5.</span> <span class="nav-text">4.5Self-Attention的输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6Multi-Head-Attention"><span class="nav-number">4.6.</span> <span class="nav-text">4.6Multi-Head Attention</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Encoder%E7%BB%93%E6%9E%84"><span class="nav-number">5.</span> <span class="nav-text">5.Encoder结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1%E7%BB%93%E6%9E%84%E5%9B%BE"><span class="nav-number">5.1.</span> <span class="nav-text">5.1结构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Add-Norm%E7%BB%93%E6%9E%84%E8%A7%A3%E9%87%8A"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 Add&amp;Norm结构解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Feed-Forward"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 Feed Forward</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-%E7%BB%84%E6%88%90Encoder"><span class="nav-number">5.4.</span> <span class="nav-text">5.4 组成Encoder</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Decoder%E7%BB%93%E6%9E%84"><span class="nav-number">6.</span> <span class="nav-text">6.Decoder结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1%E7%BB%93%E6%9E%84%E5%9B%BE"><span class="nav-number">6.1.</span> <span class="nav-text">6.1结构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2%E7%AC%AC%E4%B8%80%E4%B8%AAMulti-Head-Attention"><span class="nav-number">6.2.</span> <span class="nav-text">6.2第一个Multi-Head Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1%E5%AE%9E%E4%BE%8B%E9%A2%84%E6%B5%8B%E8%BF%87%E7%A8%8B"><span class="nav-number">6.2.1.</span> <span class="nav-text">6.2.1实例预测过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3%E7%AC%AC%E4%BA%8C%E4%B8%AA-Multi-Head-Attention"><span class="nav-number">6.3.</span> <span class="nav-text">6.3第二个 Multi-Head Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-SoftMax%E9%A2%84%E6%B5%8B%E8%BE%93%E5%87%BA%E5%8D%95%E8%AF%8D"><span class="nav-number">6.4.</span> <span class="nav-text">6.4 SoftMax预测输出单词</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Encoder%E5%92%8CDecoder%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">7.</span> <span class="nav-text">7.Encoder和Decoder的区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E6%80%BB%E7%BB%93"><span class="nav-number">8.</span> <span class="nav-text">8.总结</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
